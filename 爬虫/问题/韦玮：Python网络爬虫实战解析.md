# 韦玮：Python网络爬虫实战解析

2016年12月27日晚8点半，CSDN特邀IT专家、《Python系列实战教程》系列图书作者韦玮带来了主题为“Python网络爬虫反爬破解策略实战”的Chat交流。以下是主持人小冰对交流精彩片段的记录。

本实录中所用到的代码，均可在本地址下载：http://pan.baidu.com/s/1c2mTaVm，密码：e1tn。

## 问：能不能把使用代理池和ua代理池爬取一个网站案例完善代码给一下，有时候知道怎么写，但不知道放在哪个位置。

答：有的，豆瓣爬虫（loginpjt.zip的百度云链接）代码就是完整的，这里面应该包括了代理ip池跟用户代理池+验证码处理。

## 问： Xpath表达式在什么情况使用，例如我用urllib和Re爬取完第一级界面，然后想用Xpath做，他就实现不了。

答：做网络爬虫的时候，首先应该选用的就是Xpath表达式，因为其效率比较高。凡是XML代码基本上都可以使用Xpath表达式进行提取，若是其他信息需要提取，但是使用Xpath不能满足需求的时候，那么我们会考虑，正则表达式（即Re模块）去做。你用urllib和Re爬取完第一级界面，然后想用Xpath做，是可以实现的，只不过您的方式不对而已。如果使用urllib进行爬取的内容，请使用etree处理一下内容，即可使用Xpath进行信息提取，若使用Scrapy框架进行爬取的内容，可以直接使用Xpath表达式，不需要转换。

## 问：我用crawl爬虫时，不能够到一个指定的界面下爬取，例如贴吧里面搜索一个关键词，然后从这页面用自动爬虫，就不行。

答：可以的，同样还是方式不对的问题。可以这样做，首先观察贴吧的网址构造，一般都是使用get的请求方式进行。然后你构造出对应的网址，使用scrapy爬虫进行爬取，爬了这一个页面之后，请在这一个页面的最后，使用scrapy.http下的Request()再发起一个请求，设置一个回调函数，通过callback指定，然后在新回调函数中处理下一级页面。

## 问：正则表达式可以详细的讲讲吗？有时候不知道写成什么样的表达式去爬内容。

答：正则表达式的知识点比较多，总的来说可以分为原子、正则表达式函数、模式修正符几块的内容。您可以参考我年后即将上线的一本书《Python网络爬虫实战》第五章的内容，也可以在网上寻找一些资料，也可以听我的一些视频课程解决。

## 问：代理池是一个靠谱的思路吗？我试过就像只有1%左右有效的代理，并且速度都很慢？如果抓取量大一点，使用代理根本无法完成任务。是否有其他的方法应对IP限制？

答：代理池是一个靠谱的思路。之所以试了之后有效率在1%的左右，是因为你的代理IP没有经过筛选。经过实际中的试验，国内的代理IP大部分失效，请使用国外的代理IP，即将请求的通过get字段限制IP资源的提取即可。除此之外，稳定的IP资源还可以从这些方面获取：
- 1、从不同idc商购买云服务器，自然有IP。
- 2、通过手机网络实现IP，比如如果是公司，那么你批量购买手机卡，不同的手机卡有不同的IP。
- 3、从高校拿IP，高校IP资源多。
- 4、Adsl拨号，不建议，因为比较不稳定也比较麻烦。用国外代理IP，别用国内的。另外，实时请求，不要固定这些IP。
```python
import urllib.request
IPPOOLS=urllib.request.urlopen("http://tpv.daxiangdaili.com/ip/?tid=559126871522487&num=1&foreign=only").read().decode("utf-8","ignore")
```
通过这个进行实时请求。然后foreign代表国外IP。另外，频率要注意，一台服务器不要短时间内进行大量请求。（频率看网站而定的，一般是首先尽量用爬虫去爬他，多次观察阈值。）还有就是，ua池必须建立起来，不然很容易出问题。

## 问：现在有些验证码变成一个按钮，从左拖动到右边的方式，这种有应对方法吗？

答：有方法的，这种情况，必须用phantomjs，用phantomjs实现把滑动条拖动，然后进行登录。登录以后，把cookie传给scrapy，交由scrapy继续处理。因为phantomjs效率比较低，所以不建议全程使用。

## 问：新手刚开始玩爬虫，发现入门或者说上手很简单，但找不到有兴趣的练手及提升的项目。不知道老师是否可以介绍一些，相对容易且有趣的练手提升的项目。

答：对的，是这样。我建议你爬同程网的机票，蛮有挑战性的，我有个学员也做了一下这个实验，也可以把他的代码分享给大家看看，tongcheng.py的百度云链接。

## 问： 1）我的ip代理经常被封，有可靠的途经获得这些高质量ip代理吗。2）如何对爬虫进行监控，爬的站点太多了难免有掉队的，如何在一个范围内快速报警，通知到服务已经不行了，求监控经验。3）如果使用前端内嵌别人网站，求跨域经验。

答：1）有的，IP问题在问题5解答了。2）首先，一定要进行异常处理，异常处理可以保证你的爬虫出问题的时候，自动跳过该次，进行下一次。其次，监控爬虫目前没有开源的工具，但是，你可以用py代码，不久发一个请求，看看某些爬虫正常吗，不正常，关掉，或者自动发送消息到管理。3）将自己的代码内嵌到其他网站吗？这个达不到吧? 没有权限。

## 问： 如果想搞多线程甚至分布式爬虫，该怎么处理呢？有的时候需要爬多个网站数据，数据量涉及到TB量级，方案改如何设计呢？ 数据上来了，普通的IP线程池，太少了的话，肯定不行了，IP的线程池，该如何设计好呢？

答：数据上来以后，主要是这几个问题，
- 1. 网络数据的下载太慢；
- 2. 数据的筛选太慢；
- 3. 数据的存储太慢。此时，必须分布式，分布式请用scrapy+docker+redis的方案就能解决。Docker解决分布式处理问题，redis解决数据存储问题。另外，IP方面用刚才的IP代理就行。还有，线程安排方面，把握平衡原则就好，就是，网络数据下载、数据筛选、数据写入这三个模块的时间消耗尽量一致，若某一个模块时间消耗久，加多线程，反之亦然。

## 问：python爬虫比java爬虫 有什么优势呢？python爬虫框架用scrapy了，python的scrapy比起自己用xpath，正则等有什么优势呢？python 智能学习有哪些优势呢？python写webapi性能如何？python Django和Flask框架对比如何？该选择哪个？

答：
- 1. python比较简明，java比较严谨。综合来说，python网络处理方面是比较强悍的。
- 2. Scrapy是个框架，里面正则跟xpath都能用，xpath主要比较快，但是能力没有re强悍，xpath只能处理XML，re啥都能处理。
- 3. python 写webapi 性能很好，比如selenium。但是怎么说呢，终究还是不如scrapy、urllib等爬虫效率高。
- 4. Django通用，Flask比较轻。如果是我，通用网站我肯定用Django，小网站我可能会用Flask，综合建议Django。数据量小的话，可以直接使用数据库的数据约束进行。

## 问：随着爬虫抓取数据越来越多，去重url很重要，但是由于众多爬虫，速度需要跟的上，请问下这样的情况下可以采用什么方式来做这件事？

答：数据量大的话，可以使用布隆过滤器进行。数据约束一般unique跟primary key。但是很遗憾，目前Python3暂时没有布隆过滤器这个模块。此处参考bloom.py 和 BitVector.py 的百度云链接。布隆过滤器性能还是很高的。没办法降低，因为他就是靠牺牲准确率，来提高时间和空间的效率。而且它的报错率很低，放心。而且，不回漏报，只会误报。

## 问：老师有尝试做过对app反编译抓数据吗？抓App数据除了appium外老师有其他的一些推荐没？

答：app的数据抓取方案如下：
- 1. 抓包分析，就分析那些对应的网址，看看能不能直接构造出来。
- 2. 通过安卓模拟器解决这个问题，但是效率太低。
- 3. 通过Appium等工具，但效率吧，还是不高。所以1一定是首推。

## 问：老师现在公司是以抓取数据为主业吗？就是想了解以抓取为基础的商业业务有哪些？

答：工作目前主要以数据分析为主。以抓取为基础的商业业务有：
- 1. 数据分析行业；
- 2. 电子商务行业；
- 3. 其他需要竞争对手的数据的行业，比如去哪儿等第三方订票平台。

（以上内容转自GitChat，版权归GitChat所有，转载请联系GitChat，微信号：GitChat，原文： 《韦玮：Python网络爬虫实战解析》）